model_args:
  labels_map:
    0: 2
    1: 0
training_args:
  per_device_train_batch_size: 18
  per_device_eval_batch_size: 18
  evaluation_strategy: "steps"
  save_strategy: "epoch"
  eval_steps: 1200
  learning_rate: 0.00002
  num_train_epochs: 3
  weight_decay: 0.01
  logging_steps: 500
  warmup_steps: 128
  lr_scheduler_type: 'cosine'